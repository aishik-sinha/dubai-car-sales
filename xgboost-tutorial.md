# XGBoost: The Complete Visual Guide for Absolute Beginners
## Understanding Gradient Boosting Through Pictures and Stories

---

## Table of Contents

1. [The Big Picture: What is XGBoost?](#the-big-picture)
2. [The Story of Three Friends](#the-story)
3. [How XGBoost Actually Works](#how-it-works)
4. [Understanding Parameters Visually](#parameters-visual)
5. [Hands-On: Your First XGBoost Model](#first-model)
6. [Visual Debugging Guide](#debugging)
7. [Interactive Experiments](#experiments)

---

## The Big Picture: What is XGBoost?

### The 30-Second Explanation

**XGBoost = A team of specialists who learn from each other's mistakes**

Imagine you're trying to guess the price of used cars. Instead of asking one expert or asking many experts to vote, XGBoost does something clever:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  XGBOOST'S APPROACH:                                â”‚
â”‚                                                      â”‚
â”‚  Expert 1: Makes first guesses                      â”‚
â”‚     â†“                                               â”‚
â”‚  Expert 2: Studies Expert 1's mistakes              â”‚
â”‚            Only fixes those mistakes                â”‚
â”‚     â†“                                               â”‚
â”‚  Expert 3: Studies Expert 1 + 2's remaining errors  â”‚
â”‚            Specializes in fixing those              â”‚
â”‚     â†“                                               â”‚
â”‚  ...and so on...                                    â”‚
â”‚                                                      â”‚
â”‚  Final Answer = Expert 1 + Expert 2 + Expert 3 + ...â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**The magic**: Each new expert focuses ONLY on fixing previous mistakes, making the team incredibly efficient!

---

## The Story of Three Friends

Let me tell you a story about three friends trying to guess car prices at an auction...

### Meet the Friends

```
ğŸ‘¤ DANIEL (Decision Tree)
   - Works alone
   - Makes one big guess
   - Sometimes brilliant, sometimes way off
   
ğŸ‘¥ RACHEL (Random Forest)  
   - Brings 500 friends
   - Everyone guesses independently
   - They vote on the answer
   - Usually pretty good!
   
ğŸ¯ XAVIER (XGBoost)
   - Brings a sequential team
   - Each person fixes the previous person's mistakes
   - Often the most accurate!
```

### The Auction Challenge

A car appears: 2015 Toyota Camry, 50,000 miles

**DANIEL'S APPROACH:**
```
Daniel thinks: "Hmm, looks like a mid-range sedan"
Guess: $15,000
Actual: $18,000
Error: -$3,000 (too low!)
```

**RACHEL'S APPROACH:**
```
Rachel's 500 friends each guess:
Friend 1: $14,000
Friend 2: $16,000
Friend 3: $19,000
...
Friend 500: $17,000

They vote/average: $17,200
Actual: $18,000
Error: -$800 (pretty close!)
```

**XAVIER'S APPROACH:**
```
Round 1 - Expert A:
  Guesses: $15,000
  Error: -$3,000 (too low)
  
Round 2 - Expert B:
  Thinks: "Expert A was $3k too low, why?"
  Finds: Expert A underestimated low-mileage cars
  Correction: +$2,500
  New guess: $15,000 + $2,500 = $17,500
  Error: -$500 (better!)
  
Round 3 - Expert C:
  Thinks: "Still $500 too low, why?"
  Finds: Toyota brand is more valuable than estimated
  Correction: +$400
  New guess: $17,500 + $400 = $17,900
  Error: -$100 (very close!)
  
Round 4 - Expert D:
  Makes final tiny correction: +$100
  Final guess: $18,000
  Error: $0 (Perfect!)
```

**Xavier wins!** Each expert specialized in fixing specific mistakes.

---

## How XGBoost Actually Works

### Visual Step-by-Step Process

#### STEP 1: Initial Prediction
```
All predictions start with a simple guess:
(Usually the average or most common value)

Cars: [Car1, Car2, Car3, Car4, Car5]
True Prices: [10k, 15k, 20k, 25k, 30k]
Average: 20k

Initial Guess for ALL cars: 20k, 20k, 20k, 20k, 20k

Errors: [-10k, -5k, 0k, +5k, +10k]
         (negative = we guessed too high)
```

#### STEP 2: Build First Tree (Focuses on Errors)

```
Tree 1 asks: "Why were we wrong?"

         [Mileage < 60k?]
         /              \
      YES               NO
      /                  \
  Predict: -7k      Predict: +7k
  (reduce guess)    (increase guess)

New Predictions:
Car1 (high mileage): 20k - 7k = 13k  (was 10k, closer!)
Car2 (low mileage):  20k + 7k = 27k  (was 25k, closer!)

Remaining Errors: [-3k, -2k, 0k, +2k, +3k]  (Smaller!)
```

#### STEP 3: Build Second Tree (Fixes Remaining Errors)

```
Tree 2 asks: "What mistakes remain?"

         [Year < 2015?]
         /              \
      YES               NO
      /                  \
  Predict: -4k      Predict: +4k

Even better predictions now!
Remaining Errors: [-1k, 0k, 0k, +0.5k, +1k]  (Tiny!)
```

#### STEP 4: Continue Until Errors Are Minimal

```
Each tree makes predictions smaller and smaller:

Tree 1: Fixes big errors     (Â±7k corrections)
Tree 2: Fixes medium errors  (Â±4k corrections)  
Tree 3: Fixes small errors   (Â±2k corrections)
Tree 4: Fine-tunes           (Â±0.5k corrections)
...

Like zooming in on a target! ğŸ¯
```

### The Formula (Visual)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final Prediction =                                  â”‚
â”‚                                                       â”‚
â”‚    Initial Guess                                     â”‚
â”‚    + (Learning_Rate Ã— Tree_1_Prediction)             â”‚
â”‚    + (Learning_Rate Ã— Tree_2_Prediction)             â”‚
â”‚    + (Learning_Rate Ã— Tree_3_Prediction)             â”‚
â”‚    + ...                                             â”‚
â”‚    + (Learning_Rate Ã— Tree_N_Prediction)             â”‚
â”‚                                                       â”‚
â”‚  Learning Rate = How much we trust each tree         â”‚
â”‚                  (Usually 0.01 to 0.3)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why "Gradient" Boosting?

```
Imagine you're lost in mountains (high error) 
trying to reach a valley (zero error):

âŒ Random Walk: Try random directions
   (Decision Tree - inefficient)
   
âœ“ Gradient Descent: Always walk downhill
   (XGBoost - smart!)

        ğŸ”ï¸ High Error
        /  \
       /    \
      /   â¬‡ï¸ Follow gradient
     /    (steepest descent)
    /      \
   ğŸ•ï¸ Low Error


The "gradient" tells us the direction 
that reduces error the fastest!
```

---

## Understanding Parameters Visually

### Parameter 1: Learning Rate (eta)

**The Step Size Parameter**

```
Imagine walking to a target:

eta = 0.3 (Fast learner)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’ ğŸ¯
Step 1    Step 2    Step 3   Target (4 steps)

Pros: Fast, needs fewer trees
Cons: Might overstep target!


eta = 0.1 (Moderate learner)  
â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â†’ ğŸ¯
 1   2   3   4   5   6   7   8   9  10    (10 steps)

Pros: Balanced approach
Cons: Moderate speed


eta = 0.01 (Slow learner)
â”œâ”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â”¼â†’ ğŸ¯
(100 tiny steps)

Pros: Very precise, best accuracy
Cons: Slow, needs MANY trees
```

**Visual Impact on Learning:**

```
High eta (0.3):
  â•±â•²
 â•±  â•²    Reaches goal fast
â•±____â•²__ but might bounce around
     ğŸ¯

Low eta (0.01):
    â•±
   â•±
  â•±      Slow, steady approach
 â•±       Very precise
â•±________
        ğŸ¯
```

**Rule of Thumb:**
- eta = 0.3 â†’ need ~50-100 trees
- eta = 0.1 â†’ need ~100-300 trees  
- eta = 0.01 â†’ need ~500-1000 trees

---

### Parameter 2: max_depth (Tree Depth)

**How Many Questions Can Each Tree Ask?**

```
max_depth = 2 (SHALLOW - XGBoost prefers this!)

              [Root]
             /      \
        [Level 1]  [Level 1]
        /      \    /      \
      [L2]    [L2][L2]    [L2]
      â†“       â†“   â†“       â†“
    Predict Predict...

Only 2 questions per path
Simple patterns only
Less overfitting âœ“


max_depth = 6 (MODERATE)

              [Root]
             /      \
          [...]    [...]
          â†™  â†˜    â†™  â†˜
        [...]...[...]
        (Many levels)
        
More complex patterns
Can capture details
Risk of overfitting âš ï¸


max_depth = 20 (TOO DEEP!)

              [Root]
          â•±â•±â•±â•± â•²â•²â•²â•²
        [Extremely complex tree]
        [Memorizes training data]
        [Won't work on new data]
        
OVERFITTING! âŒ
```

**Why XGBoost Uses Shallow Trees:**

```
Random Forest Logic:
"Each tree must be smart individually"
â†’ Needs deep trees

XGBoost Logic:
"Each tree just fixes one specific mistake"
â†’ Shallow trees are enough
â†’ 100 shallow trees > 10 deep trees


Analogy:
ğŸŒ³ One deep tree = One genius doing everything
ğŸŒ±ğŸŒ±ğŸŒ± Many shallow trees = Team of specialists
```

**Best Practices:**
- Start with: max_depth = 6
- Try: 3, 4, 5, 6, 8
- Rarely need: > 10

---

### Parameter 3: subsample

**How Much Data Does Each Tree See?**

```
subsample = 1.0 (Use ALL data)

Tree 1 sees: [ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—] (all 10 cars)
Tree 2 sees: [ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—] (all 10 cars)
Tree 3 sees: [ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—] (all 10 cars)

Problem: Trees might learn similar patterns
Risk: Overfitting


subsample = 0.8 (Use 80% randomly)

Tree 1 sees: [ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—--] (8 random cars)
Tree 2 sees: [ğŸš—-ğŸš—ğŸš—ğŸš—ğŸš—-ğŸš—ğŸš—ğŸš—] (different 8 cars)
Tree 3 sees: [-ğŸš—ğŸš—-ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—] (different 8 cars)

Benefit: Each tree learns slightly different patterns
Result: Better generalization âœ“


subsample = 0.5 (Use only 50%)

Tree 1 sees: [ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—-----] (only 5 cars)
Tree 2 sees: [--ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—---] (different 5)

Too little data per tree!
Might miss important patterns âŒ
```

**Think of it like:**
```
subsample = 0.8 is like:
- Studying with different practice problems each time
- You learn more robust patterns
- Less likely to memorize specific examples
```

**Recommended:**
- Start with: 0.8
- Try: 0.7, 0.8, 0.9, 1.0
- Below 0.6: Usually too low

---

### Parameter 4: colsample_bytree

**How Many Features Does Each Tree Use?**

```
Available Features: [Price, Mileage, Year, Brand, Color]


colsample_bytree = 1.0 (Use ALL features)

Tree 1: [Price, Mileage, Year, Brand, Color] âœ“âœ“âœ“âœ“âœ“
Tree 2: [Price, Mileage, Year, Brand, Color] âœ“âœ“âœ“âœ“âœ“
Tree 3: [Price, Mileage, Year, Brand, Color] âœ“âœ“âœ“âœ“âœ“

Risk: All trees might focus on same features


colsample_bytree = 0.8 (Use 80% of features)

Tree 1: [Price, Mileage, Year, Brand, ----] âœ“âœ“âœ“âœ“
Tree 2: [Price, -----, Year, Brand, Color] âœ“-âœ“âœ“âœ“
Tree 3: [----, Mileage, Year, Brand, Color] âœ“âœ“âœ“âœ“

Better: Each tree explores different feature combinations!


colsample_bytree = 0.5 (Use 50% of features)

Tree 1: [Price, Mileage, ----] âœ“âœ“
Tree 2: [----, Year, Brand] âœ“âœ“
Tree 3: [Price, ----, Color] âœ“âœ“

More diversity, but might miss important feature combinations
```

**Visual Analogy:**

```
Imagine solving a puzzle:

colsample = 1.0:
Everyone sees ALL pieces
â†’ Might all try same approach

colsample = 0.8:
Each person sees MOST pieces
â†’ Try different approaches
â†’ Together cover everything âœ“

colsample = 0.5:
Each person sees HALF the pieces  
â†’ Might miss important connections
```

**Best Practice:**
- Start with: 0.8
- Range: 0.5 to 1.0
- Like Random Forest's mtry parameter!

---

### Parameter Summary Table

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Parameter      â”‚   Controls   â”‚  Typical Range  â”‚    Advice    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ eta              â”‚ Learning     â”‚ 0.01 - 0.3      â”‚ Lower = Betterâ”‚
â”‚ (learning_rate)  â”‚ speed        â”‚ Start: 0.1      â”‚ (but slower) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ max_depth        â”‚ Tree         â”‚ 3 - 10          â”‚ Shallow wins!â”‚
â”‚                  â”‚ complexity   â”‚ Start: 6        â”‚ 3-8 typical  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ subsample        â”‚ Data         â”‚ 0.5 - 1.0       â”‚ 0.8 is sweet â”‚
â”‚                  â”‚ sampling     â”‚ Start: 0.8      â”‚ spot         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ colsample_bytree â”‚ Feature      â”‚ 0.5 - 1.0       â”‚ More featuresâ”‚
â”‚                  â”‚ sampling     â”‚ Start: 0.8      â”‚ = OK here    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ nrounds          â”‚ Number of    â”‚ 50 - 1000       â”‚ Depends on   â”‚
â”‚                  â”‚ trees        â”‚ Start: 100      â”‚ eta value    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Hands-On: Your First XGBoost Model

### The Simplest Possible Example

```r
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# BABY'S FIRST XGBOOST MODEL
# Copy and run this!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Load library
library(xgboost)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 1: Create tiny example data
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

cat("Creating example data: Predicting if a car is Expensive...\n\n")

# Features: Price and Mileage
car_data <- data.frame(
  Price = c(5000, 8000, 15000, 18000, 25000, 30000, 35000, 40000),
  Mileage = c(120000, 100000, 80000, 70000, 50000, 40000, 30000, 20000)
)

# Labels: 0 = Cheap, 1 = Expensive
# (Cars over $20k are expensive)
car_labels <- c(0, 0, 0, 0, 1, 1, 1, 1)

# Visualize the data
cat("Our Data:\n")
print(cbind(car_data, Expensive = ifelse(car_labels == 1, "Yes", "No")))

cat("\nPattern: Low mileage + high price = Expensive car\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 2: Convert to XGBoost format
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# XGBoost needs a MATRIX (not data frame)
features_matrix <- as.matrix(car_data)

# Create DMatrix (XGBoost's special format)
dtrain <- xgb.DMatrix(data = features_matrix, label = car_labels)

cat("âœ“ Data converted to XGBoost format!\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 3: Set parameters
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

params <- list(
  objective = "binary:logistic",  # Predicting 0 or 1
  max_depth = 3,                  # Small tree
  eta = 0.3                       # Learning rate
)

cat("Parameters set:\n")
cat("  - Binary classification (Cheap vs Expensive)\n")
cat("  - max_depth = 3 (small tree)\n")
cat("  - eta = 0.3 (moderate learning rate)\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 4: Train the model
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

cat("Training model...\n\n")

set.seed(42)
model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 10,            # Just 10 trees
  verbose = 1              # Show progress
)

cat("\nâœ“ Model trained!\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 5: Make predictions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

predictions <- predict(model, features_matrix)
predicted_class <- ifelse(predictions > 0.5, "Expensive", "Cheap")

cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
cat("           RESULTS\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")

results <- data.frame(
  Price = car_data$Price,
  Mileage = car_data$Mileage,
  Actual = ifelse(car_labels == 1, "Expensive", "Cheap"),
  Predicted = predicted_class,
  Probability = round(predictions, 3),
  Correct = ifelse(predicted_class == ifelse(car_labels == 1, "Expensive", "Cheap"), 
                   "âœ“", "âœ—")
)

print(results)

accuracy <- mean(predicted_class == ifelse(car_labels == 1, "Expensive", "Cheap"))
cat("\nAccuracy:", round(accuracy * 100, 1), "%\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 6: Understand what the model learned
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

cat("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
cat("      WHAT DID THE MODEL LEARN?\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")

importance <- xgb.importance(model = model, feature_names = c("Price", "Mileage"))
print(importance)

cat("\nInterpretation:\n")
if (importance$Feature[1] == "Price") {
  cat("  â†’ Price is MORE important than Mileage\n")
  cat("  â†’ Makes sense: Expensive cars have high prices!\n")
} else {
  cat("  â†’ Mileage is MORE important than Price\n")
  cat("  â†’ Interesting: Low mileage indicates expensive cars!\n")
}

cat("\nğŸ‰ Congratulations! You just built your first XGBoost model!\n")
```

### Understanding the Output

When you run this code, you'll see:

```
[1]	train-logloss:0.598438
[2]	train-logloss:0.516234
[3]	train-logloss:0.451289
...

What does this mean?

logloss = Logarithmic loss (error measure)
Lower = Better!

[1] = After tree 1: error = 0.598 (starting point)
[2] = After tree 2: error = 0.516 (improving! â†“)
[3] = After tree 3: error = 0.451 (still improving! â†“)

If error stops decreasing â†’ model has learned all it can
If error increases â†’ overfitting! âš ï¸
```

---

## Visual Debugging Guide

### Problem 1: Training vs Validation Error

**The Learning Curve (Most Important Graph!)**

```
Good Learning (Healthy Model):
Error
 â”‚
 â”‚  Training â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•²___________
 â”‚                                       
 â”‚  Validation â”â”â”â”â”â”â”â”â”â”â”â”â”â•²__________
 â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Trees
 
 Both decrease together âœ“
 Small gap âœ“
 Both flatten out âœ“


Overfitting (Problem!):
Error
 â”‚
 â”‚  Training â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•²â•²â•²â•²â•²â•²___
 â”‚                                â†“â†“â†“â†“
 â”‚  Validation â”â”â”â”â”â”â”â”â”â•²â•±â•²â•±â•²â•±â•²â•±
 â”‚                        â†‘ Going up!
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Trees
 
 Training improves âœ“
 Validation gets worse âœ—
 BIG gap âœ—
 
 Solution:
 - Reduce max_depth
 - Lower eta
 - Increase subsample
 - Stop earlier!


Underfitting (Also a problem):
Error
 â”‚
 â”‚  Training â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 â”‚           (stays high)
 â”‚  Validation â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
 â”‚             (also stays high)
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Trees
 
 Both high, not improving
 
 Solution:
 - Increase max_depth
 - Increase eta
 - Add more trees
 - Add more features
```

### Problem 2: Feature Importance Doesn't Make Sense

```
Expected Importance:
Price: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 80%
Mileage: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 20%

Actual Importance:
Color: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 60%
Price: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 20%
Mileage: â–ˆâ–ˆâ–ˆâ–ˆ 20%

Something's wrong! ğŸš¨

Possible Causes:

1. Data Leakage:
   Color = "red" for all expensive cars
   Color = "blue" for all cheap cars
   â†’ Model cheats by using color!
   
2. Random Correlation:
   By chance, some weird feature correlates
   â†’ Check if it makes logical sense
   
3. Wrong Feature Engineering:
   Maybe you logged the wrong feature
   Or created a feature that's too perfect
   
4. Target Leakage:
   A feature that includes the answer!
   Example: "Price_Category" in a price prediction model

How to Debug:
âœ“ Remove suspicious feature and retrain
âœ“ Check correlation: cor(features)
âœ“ Ask: "Would this feature exist for NEW cars?"
âœ“ Use common sense!
```

### Problem 3: Predictions Are All The Same Class

```
Your Predictions:
Good: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 280
Average: â–ˆ 10
Bad: â–ˆ 10

Uh oh! Model always predicts "Good"! ğŸ˜±

Visual Diagnosis:

Your Data:
Good: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 120 samples
Average: â–ˆâ–ˆâ–ˆ 30 samples  
Bad: â–ˆâ–ˆâ–ˆ 30 samples

Problem: IMBALANCED CLASSES!

Why This Happens:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model's thinking:                   â”‚
â”‚ "If I always guess 'Good',          â”‚
â”‚  I'm right 120/180 times (67%)!     â”‚
â”‚  Why bother learning patterns?"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Solutions:

1. Use scale_pos_weight:
   Penalize mistakes on rare classes more
   
2. Oversample rare classes:
   Duplicate "Average" and "Bad" samples
   
3. Undersample common class:
   Use fewer "Good" samples
   
4. Use stratified sampling:
   Ensure each fold has balanced classes

Code Example:
# Calculate class weights
neg_count <- sum(labels == 0)
pos_count <- sum(labels == 1)
scale_weight <- neg_count / pos_count

params$scale_pos_weight <- scale_weight
```

---

## Interactive Experiments

### Experiment 1: Learning Rate Explorer

**Try This Code:**

```r
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXPERIMENT: How Does eta Affect Learning?
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

library(xgboost)
library(ggplot2)

# Create simple data
set.seed(42)
n <- 100
X <- matrix(rnorm(n * 2), ncol = 2)
y <- ifelse(X[,1] + X[,2] > 0, 1, 0)
dtrain <- xgb.DMatrix(data = X, label = y)

# Test different learning rates
eta_values <- c(0.01, 0.05, 0.1, 0.3)
results_list <- list()

cat("Testing different learning rates...\n\n")

for (eta_val in eta_values) {
  cat("Training with eta =", eta_val, "...\n")
  
  params <- list(
    objective = "binary:logistic",
    max_depth = 3,
    eta = eta_val,
    eval_metric = "error"
  )
  
  # Train and record progress
  model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 100,
    verbose = 0
  )
  
  # Store results
  results_list[[as.character(eta_val)]] <- model$evaluation_log
}

# Plot results
plot_data <- do.call(rbind, lapply(names(results_list), function(eta) {
  df <- results_list[[eta]]
  df$eta <- paste("eta =", eta)
  df
}))

ggplot(plot_data, aes(x = iter, y = train_error, color = eta)) +
  geom_line(size = 1.2) +
  labs(
    title = "How Learning Rate Affects Training",
    subtitle = "Lower eta = slower but steadier learning",
    x = "Number of Trees",
    y = "Error Rate",
    color = "Learning Rate"
  ) +
  theme_minimal() +
  theme(legend.position = "top")

cat("\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
cat("           OBSERVATIONS\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")
cat("Look at the graph and notice:\n\n")
cat("1. High eta (0.3):\n")
cat("   - Drops fast initially\n")
cat("   - Might bounce around\n")
cat("   - Needs fewer trees\n\n")
cat("2. Low eta (0.01):\n")
cat("   - Drops slowly and steadily\n")
cat("   - Very smooth curve\n")
cat("   - Needs many more trees\n\n")
cat("3. Medium eta (0.1):\n")
cat("   - Good balance\n")
cat("   - Usually the sweet spot!\n")
```

### Experiment 2: Tree Depth Explorer

```r
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXPERIMENT: How Does max_depth Affect Model?
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

library(xgboost)

# Create data with different complexity levels
set.seed(42)
n <- 200

# Simple pattern
X_simple <- matrix(rnorm(n * 2), ncol = 2)
y_simple <- ifelse(X_simple[,1] > 0, 1, 0)

# Complex pattern
X_complex <- matrix(rnorm(n * 5), ncol = 5)
y_complex <- ifelse(
  (X_complex[,1] > 0 & X_complex[,2] > 0) |
  (X_complex[,3] > 0 & X_complex[,4] < 0), 1, 0
)

test_depths <- function(X, y, data_name) {
  cat("\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
  cat("  Testing:", data_name, "\n")
  cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")
  
  dtrain <- xgb.DMatrix(data = X, label = y)
  
  depths <- c(1, 2, 3, 6, 10, 15)
  
  for (depth in depths) {
    params <- list(
      objective = "binary:logistic",
      max_depth = depth,
      eta = 0.1
    )
    
    # Cross-validation
    cv_results <- xgb.cv(
      params = params,
      data = dtrain,
      nrounds = 50,
      nfold = 5,
      verbose = 0
    )
    
    train_error <- cv_results$evaluation_log$train_error_mean[50]
    test_error <- cv_results$evaluation_log$test_error_mean[50]
    gap <- test_error - train_error
    
    cat("Depth =", sprintf("%2d", depth), "â”‚")
    cat(" Train Error:", sprintf("%.3f", train_error), "â”‚")
    cat(" Test Error:", sprintf("%.3f", test_error), "â”‚")
    cat(" Gap:", sprintf("%.3f", gap))
    
    if (gap < 0.05) {
      cat(" âœ“ Good fit\n")
    } else if (gap < 0.1) {
      cat(" âš  Slight overfit\n")
    } else {
      cat(" âœ— Overfitting!\n")
    }
  }
}

# Test on simple data
test_depths(X_simple, y_simple, "SIMPLE PATTERN")

# Test on complex data
test_depths(X_complex, y_complex, "COMPLEX PATTERN")

cat("\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
cat("           KEY INSIGHTS\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")
cat("Simple Pattern:\n")
cat("  - Shallow trees (depth 2-3) work great!\n")
cat("  - Deep trees overfit (big gap)\n")
cat("  - Keep it simple! âœ“\n\n")
cat("Complex Pattern:\n")
cat("  - Need deeper trees (depth 6+)\n")
cat("  - But not TOO deep (15 is overkill)\n")
cat("  - Match depth to problem complexity\n")
```

### Experiment 3: Watch XGBoost Learn in Real-Time

```r
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXPERIMENT: Visualize the Learning Process
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

library(xgboost)
library(ggplot2)

# Create clear visual data
set.seed(123)
n <- 100
x <- seq(-3, 3, length.out = n)
y_true <- sin(x) * 2
y_noisy <- y_true + rnorm(n, 0, 0.5)

# Prepare data
X_matrix <- matrix(x, ncol = 1)
dtrain <- xgb.DMatrix(data = X_matrix, label = y_noisy)

# Parameters
params <- list(
  objective = "reg:squarederror",
  max_depth = 3,
  eta = 0.1
)

cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
cat("    WATCHING XGBOOST LEARN\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")
cat("Problem: Fit a curve to noisy data\n")
cat("Watch how predictions improve tree by tree!\n\n")

# Store predictions at different stages
stages <- c(1, 5, 10, 20, 50, 100)
predictions_list <- list()

for (n_trees in stages) {
  model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = n_trees,
    verbose = 0
  )
  
  preds <- predict(model, X_matrix)
  predictions_list[[as.character(n_trees)]] <- preds
  
  cat("After", sprintf("%3d", n_trees), "trees: Error =", 
      sprintf("%.4f", mean((preds - y_noisy)^2)), "\n")
}

# Create visualization data
plot_data <- data.frame(
  x = rep(x, length(stages)),
  y_true = rep(y_true, length(stages)),
  y_noisy = rep(y_noisy, length(stages)),
  y_pred = unlist(predictions_list),
  stage = rep(paste(stages, "trees"), each = n)
)

# Plot
ggplot(plot_data) +
  geom_point(aes(x = x, y = y_noisy), alpha = 0.3, color = "gray") +
  geom_line(aes(x = x, y = y_true, color = "True Pattern"), 
            size = 1, linetype = "dashed") +
  geom_line(aes(x = x, y = y_pred, color = "XGBoost Prediction"), 
            size = 1.2) +
  facet_wrap(~stage, ncol = 3) +
  labs(
    title = "XGBoost Learning Process",
    subtitle = "Watch the red line get closer to the blue line!",
    x = "Input Feature",
    y = "Prediction",
    color = ""
  ) +
  theme_minimal() +
  theme(legend.position = "top")

cat("\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
cat("           WHAT YOU SEE\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")
cat("1 tree:    Very rough approximation\n")
cat("5 trees:   Starting to see the pattern\n")
cat("10 trees:  Getting the general shape\n")
cat("20 trees:  Close to the true curve\n")
cat("50 trees:  Very accurate fit\n")
cat("100 trees: Might be overfitting to noise!\n\n")
cat("This is gradient boosting in action! ğŸš€\n")
```

---

## Advanced Concepts (Simplified!)

### Concept 1: Regularization (Preventing Overfitting)

**Think of it like handwriting:**

```
No Regularization:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â•±â•² â•±â•²â•±â•² â•±â•²â•±â•²â•±â•²      â”‚  Too detailed
â”‚â•±  â•²   â•±  â•²  â•² â•²     â”‚  Follows every wiggle
â”‚            â•²  â•²     â”‚  Memorizing!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

With Regularization:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â•±â”€â”€â”€â”€â•²              â”‚  Smooth
â”‚ â•±      â•²             â”‚  General pattern
â”‚â•±        â•²            â”‚  Better for new data!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**XGBoost Regularization Parameters:**

```r
params <- list(
  alpha = 0,        # L1 regularization (Lasso)
  lambda = 1,       # L2 regularization (Ridge)
  
  gamma = 0,        # Minimum loss reduction for split
  
  min_child_weight = 1   # Minimum data in leaf node
)

alpha (L1):
  High alpha â†’ Sparse model (many features set to 0)
  Like Marie Kondo: "Does this feature spark joy? No? Remove it!"
  
lambda (L2):
  High lambda â†’ Shrinks all weights
  Like a volume knob: turns everything down a bit
  
gamma:
  High gamma â†’ Only split if it helps A LOT
  Prevents tiny, useless splits
  
min_child_weight:
  High value â†’ Need more data per leaf
  Prevents overfitting to rare cases
```

**Visual Impact:**

```
No Regularization (alpha=0, lambda=0):
Tree splits everywhere!
â”œâ”€ Price < 10k
â”‚  â”œâ”€ Mileage < 50k
â”‚  â”‚  â”œâ”€ Color = Red  (only 2 samples!)
â”‚  â”‚  â””â”€ Color = Blue (only 1 sample!)
â”‚  â””â”€ Mileage >= 50k
â””â”€ Price >= 10k

With Regularization (alpha=1, lambda=1):
Only important splits
â”œâ”€ Price < 10k
â”‚  â””â”€ Predict: Cheap
â””â”€ Price >= 10k
   â””â”€ Predict: Expensive

Cleaner! Less overfitting! âœ“
```

---

### Concept 2: Early Stopping (The Smart Stop)

**The Story:**

```
Without Early Stopping:
You: "Train for 1000 rounds!"
Model: "Okay!" 

Round 1-50:  Getting better! â†—
Round 51-100: Still improving â†—
Round 101-200: Barely improving â†’
Round 201-1000: Actually getting WORSE on validation! â†˜

You: "Why didn't you tell me to stop?!"


With Early Stopping:
You: "Train for 1000 rounds, but stop if no improvement for 10 rounds"
Model: "Got it!"

Round 1-50:  Getting better! â†—
Round 51-100: Still improving â†—
Round 101-110: No improvement for 10 rounds
Model: "I'm done! Best was at round 95."

You: "Smart! Saved time and prevented overfitting!"
```

**Code Example:**

```r
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMONSTRATION: Early Stopping
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Split data
set.seed(42)
train_idx <- sample(1:nrow(X), 0.8 * nrow(X))
X_train <- X[train_idx, ]
y_train <- y[train_idx]
X_val <- X[-train_idx, ]
y_val <- y[-train_idx]

dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dval <- xgb.DMatrix(data = X_val, label = y_val)

params <- list(
  objective = "binary:logistic",
  max_depth = 6,
  eta = 0.1
)

cat("Training WITH early stopping...\n\n")

model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 200,              # Max rounds
  watchlist = list(
    train = dtrain,
    validation = dval
  ),
  early_stopping_rounds = 10,  # Stop if no improvement for 10 rounds
  verbose = 1,
  print_every_n = 10
)

cat("\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
cat("Best iteration:", model$best_iteration, "\n")
cat("Best score:", model$best_score, "\n")
cat("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
cat("\nEarly stopping saved us from training", 
    200 - model$best_iteration, "unnecessary rounds!\n")
```

---

### Concept 3: Handling Missing Data

**XGBoost's Superpower:**

```
Traditional Machine Learning:
Missing value in "Mileage"? â†’ Error! âŒ
Solution: Fill with mean/median/mode

XGBoost:
Missing value? â†’ "I'll figure it out!" âœ“


How XGBoost Handles Missing Data:

         [Mileage < 50k?]
         /      |      \
      YES    MISSING   NO
       â†“       â†“        â†“
   Predict  ???     Predict

XGBoost tries BOTH paths:
- Send missing values LEFT â†’ Calculate error
- Send missing values RIGHT â†’ Calculate error
- Choose the path with LOWER error!


Example:
If cars with missing mileage tend to be:
  - Expensive â†’ Send missing values to "NO" branch
  - Cheap â†’ Send missing values to "YES" branch

Smart! It learns the pattern! ğŸ§ 
```

**Code Example:**

```r
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEMONSTRATION: Missing Data Handling
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Create data with missing values
set.seed(42)
n <- 100
X_complete <- matrix(rnorm(n * 3), ncol = 3)
y <- ifelse(rowSums(X_complete) > 0, 1, 0)

# Randomly remove 20% of values
X_missing <- X_complete
missing_mask <- matrix(runif(n * 3) < 0.2, ncol = 3)
X_missing[missing_mask] <- NA

cat("Original data: No missing values\n")
cat("Modified data:", sum(is.na(X_missing)), "missing values\n\n")

# XGBoost can handle this directly!
dtrain_missing <- xgb.DMatrix(data = X_missing, label = y)

params <- list(
  objective = "binary:logistic",
  max_depth = 3,
  eta = 0.1
)

cat("Training on data with missing values...\n")

model_missing <- xgb.train(
  params = params,
  data = dtrain_missing,
  nrounds = 50,
  verbose = 0
)

# Make predictions
preds <- predict(model_missing, X_missing)
accuracy <- mean((preds > 0.5) == y)

cat("\nâœ“ Model trained successfully!\n")
cat("Accuracy:", round(accuracy * 100, 1), "%\n\n")
cat("XGBoost learned where to send missing values automatically!\n")
```

---

## Real-World Tips and Tricks

### Tip 1: Start Simple, Then Optimize

```
âŒ Wrong Approach:
1. Load data
2. Immediately tune 10 parameters with grid search
3. Wait 3 hours
4. Get confused by results

âœ“ Right Approach:
1. Load data
2. Build SIMPLE model with defaults
   params <- list(
     objective = "multi:softmax",
     num_class = 3,
     max_depth = 6,
     eta = 0.1
   )
3. Check if it beats baseline (< 2 minutes)
4. IF it works: THEN optimize parameters
5. Change ONE parameter at a time
6. Understand what each change does


The Learning Path:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: Default Parameters              â”‚
â”‚         5-fold CV                       â”‚
â”‚         Accuracy: 75%                   â”‚
â”‚         Time: 1 minute                  â”‚
â”‚         âœ“ Beats baseline (60%)!         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Step 2: Tune eta (0.01, 0.05, 0.1, 0.3)â”‚
â”‚         Best: eta = 0.05                â”‚
â”‚         Accuracy: 78%                   â”‚
â”‚         Time: 3 minutes                 â”‚
â”‚         âœ“ Improvement!                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Step 3: Tune max_depth (3,4,5,6,8)     â”‚
â”‚         Best: max_depth = 4             â”‚
â”‚         Accuracy: 80%                   â”‚
â”‚         Time: 4 minutes                 â”‚
â”‚         âœ“ Getting better!               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Step 4: Fine-tune subsample/colsample  â”‚
â”‚         Best: 0.8 / 0.8                 â”‚
â”‚         Accuracy: 81%                   â”‚
â”‚         Time: 5 minutes                 â”‚
â”‚         âœ“ Marginal improvement          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total time: ~13 minutes
Total improvement: 60% â†’ 81% (+21 points!)
```

### Tip 2: Monitor Your Learning Curves

```r
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ALWAYS plot learning curves!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

plot_learning_curve <- function(cv_results) {
  log_data <- cv_results$evaluation_log
  
  ggplot(log_data, aes(x = iter)) +
    geom_line(aes(y = train_error_mean, color = "Training"), size = 1) +
    geom_ribbon(aes(ymin = train_error_mean - train_error_std,
                    ymax = train_error_mean + train_error_std),
                alpha = 0.2, fill = "blue") +
    geom_line(aes(y = test_error_mean, color = "Validation"), size = 1) +
    geom_ribbon(aes(ymin = test_error_mean - test_error_std,
                    ymax = test_error_mean + test_error_std),
                alpha = 0.2, fill = "red") +
    labs(
      title = "Learning Curve",
      x = "Number of Trees",
      y = "Error Rate",
      color = "Dataset"
    ) +
    theme_minimal()
}

# Use it after every CV run!
# plot_learning_curve(cv_results)
```

**What to Look For:**

```
ğŸŸ¢ Healthy Model:
- Both lines decreasing
- Small gap between train/validation
- Both flatten at the end

ğŸŸ¡ Needs More Training:
- Both lines still decreasing
- Haven't flattened yet
- â†’ Increase nrounds!

ğŸ”´ Overfitting:
- Training keeps decreasing
- Validation increases or stays flat
- Big gap
- â†’ Reduce complexity!

ğŸ”´ Underfitting:
- Both lines high
- Not decreasing much
- â†’ Increase complexity!
```

### Tip 3: Feature Engineering > Parameter Tuning

```
Time Investment vs Impact:

Feature Engineering (1 hour):
â”œâ”€ Create log transformations
â”œâ”€ Create interaction features  
â”œâ”€ Remove correlated features
â””â”€ Handle missing values intelligently
   â†’ Accuracy: 60% â†’ 75% (+15 points!) ğŸ‰

Parameter Tuning (3 hours):
â”œâ”€ Grid search over eta
â”œâ”€ Grid search over max_depth
â”œâ”€ Grid search over subsample
â””â”€ Grid search over colsample
   â†’ Accuracy: 75% â†’ 78% (+3 points) ğŸ˜


The Rule:
ğŸŒŸ Better features > Fancier models
ğŸŒŸ Understanding data > Tweaking parameters
ğŸŒŸ Simple model + good features > Complex model + raw features


Example Features for Car Data:
# Basic
Price, Mileage

# Log Transforms (handle wide ranges)
log(Price), log(Mileage)

# Ratios (capture relationships)
Price per Mile = Price / Mileage
Depreciation Rate = (Original_Price - Price) / Age

# Interactions (combined effects)
log(Price) Ã— log(Mileage)

# Domain Knowledge (think like a buyer!)
Is_Luxury = Brand in ["Mercedes", "BMW", "Lexus"]
High_Mileage = Mileage > 100000
Recently_Listed = Days_On_Market < 7
```

### Tip 4: Cross-Validation is Your Friend

```r
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ALWAYS use cross-validation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# âŒ BAD: Single train/test split
train_idx <- sample(1:nrow(data), 0.8 * nrow(data))
# Problem: Results depend on this ONE random split!

# âœ“ GOOD: Cross-validation
cv_results <- xgb.cv(
  params = params,
  data = dtrain,
  nrounds = 100,
  nfold = 5,    # 5 different train/test splits
  verbose = 0
)
# Result: More reliable estimate!


Why Cross-Validation Matters:

Single Split:
You might get lucky or unlucky!
Run 1: 82% accuracy (lucky test set)
Run 2: 71% accuracy (hard test set)
Run 3: 77% accuracy
â†’ Which is the "real" accuracy? ğŸ¤·

5-Fold CV:
Average across 5 splits:
Fold 1: 76%
Fold 2: 78%
Fold 3: 75%
Fold 4: 77%
Fold 5: 79%
Mean: 77% Â± 1.4%
â†’ More reliable! âœ“


Stratified CV (for imbalanced data):
Ensures each fold has same class distribution
Normal: [Good: 80%, Bad: 20%] in training
Stratified: [Good: 80%, Bad: 20%] in EACH fold
```

---

## Quick Reference Cheat Sheet

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              XGBOOST QUICK REFERENCE                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BASIC WORKFLOW:
  1. Prepare data â†’ matrix format
  2. Set parameters â†’ start simple
  3. Cross-validate â†’ check performance
  4. Tune parameters â†’ one at a time
  5. Train final model â†’ use all data
  6. Predict â†’ same features as training!

ESSENTIAL PARAMETERS:
  objective       What to predict
  â”œâ”€ binary:logistic     Two classes (0/1)
  â”œâ”€ multi:softmax       Multiple classes
  â””â”€ reg:squarederror    Numbers
  
  eta             Learning rate (0.01-0.3)
  â”œâ”€ Lower = Better accuracy, slower
  â””â”€ Higher = Faster, might overfit
  
  max_depth       Tree depth (3-8 typical)
  â”œâ”€ Shallow = Less overfitting
  â””â”€ Deep = More complex patterns
  
  subsample       Data sampling (0.7-1.0)
  colsample_bytree  Feature sampling (0.7-1.0)
  
  nrounds         Number of trees (50-500)

GOOD STARTING POINT:
  params <- list(
    objective = "multi:softmax",
    num_class = 3,
    eta = 0.1,
    max_depth = 6,
    subsample = 0.8,
    colsample_bytree = 0.8
  )

DEBUGGING CHECKLIST:
  â–¡ Data is matrix?
  â–¡ Labels start at 0?
  â–¡ Same features in train/test?
  â–¡ Checked for missing values?
  â–¡ Set random seed?
  â–¡ Using cross-validation?
  â–¡ Plotted learning curve?
  â–¡ Feature importance makes sense?

COMMON ERRORS & FIXES:
  "Invalid label" â†’ Labels must be 0,1,2... not 1,2,3...
  "Matrix required" â†’ Use as.matrix()
  Overfitting â†’ Lower max_depth, eta, or subsample
  All same prediction â†’ Check class balance
  Slow training â†’ Increase eta or reduce nrounds

PERFORMANCE TIPS:
  âš¡ Use nthread parameter for parallel processing
  âš¡ Start with small nrounds for testing
  âš¡ Use early_stopping_rounds
  âš¡ Sample data for parameter tuning
  
REMEMBER:
  ğŸ¯ Start simple, optimize later
  ğŸ“Š Always plot learning curves
  ğŸ” Feature engineering > parameter tuning
  âœ… Cross-validation is mandatory
  ğŸ¤” If results seem too good, check for leakage!

â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## Final Project: Put It All Together

```r
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FINAL PROJECT: Complete XGBoost Pipeline
# Copy this template for any project!
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

library(xgboost)
library(caret)
library(ggplot2)

cat("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n")
cat("â•‘      XGBOOST COMPLETE PIPELINE TEMPLATE               â•‘\n")
cat("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 1: Load Your Data
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat("Step 1: Loading data...\n")

# Replace with your data:
# train_data <- read.csv("your_data.csv")

# For demonstration:
set.seed(42)
n <- 500
train_data <- data.frame(
  feature1 = rnorm(n),
  feature2 = rnorm(n),
  feature3 = rnorm(n),
  target = sample(c("Good", "Average", "Bad"), n, replace = TRUE)
)

cat("âœ“ Data loaded:", nrow(train_data), "rows\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 2: Exploratory Data Analysis
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat("Step 2: Exploring data...\n")

cat("Target distribution:\n")
print(table(train_data$target))
cat("\nMissing values:\n")
print(colSums(is.na(train_data)))
cat("\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 3: Feature Engineering
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat("Step 3: Engineering features...\n")

# Add your feature engineering here!
# Examples:
# train_data$log_feature1 <- log(train_data$feature1 + 1)
# train_data$ratio <- train_data$feature1 / train_data$feature2
# train_data$interaction <- train_data$feature1 * train_data$feature2

cat("âœ“ Features engineered\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 4: Prepare XGBoost Format
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat("Step 4: Preparing XGBoost format...\n")

# Select features (exclude target)
feature_names <- setdiff(names(train_data), "target")
features_matrix <- as.matrix(train_data[, feature_names])

# Convert target to numeric (0, 1, 2...)
target_labels <- as.integer(as.factor(train_data$target)) - 1
label_mapping <- data.frame(
  Original = levels(as.factor(train_data$target)),
  Numeric = 0:(length(levels(as.factor(train_data$target)))-1)
)

cat("Label mapping:\n")
print(label_mapping)

# Create DMatrix
dtrain <- xgb.DMatrix(data = features_matrix, label = target_labels)

cat("âœ“ Data prepared for XGBoost\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 5: Establish Baseline
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat("Step 5: Establishing baseline...\n")

baseline_accuracy <- max(table(target_labels)) / length(target_labels) * 100
cat("Baseline (guess most common):", round(baseline_accuracy, 1), "%\n")
cat("Goal: Beat", round(baseline_accuracy + 10, 1), "%\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 6: Cross-Validation with Default Parameters
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat("Step 6: Testing default parameters...\n")

params_default <- list(
  objective = "multi:softmax",
  num_class = length(unique(target_labels)),
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

set.seed(42)
cv_default <- xgb.cv(
  params = params_default,
  data = dtrain,
  nrounds = 100,
  nfold = 5,
  metrics = "merror",
  verbose = 0,
  early_stopping_rounds = 10
)

default_error <- cv_default$evaluation_log[cv_default$best_iteration, "test_merror_mean"]
default_accuracy <- (1 - default_error) * 100

cat("Default CV Accuracy:", round(default_accuracy, 1), "%\n")
cat("Best iteration:", cv_default$best_iteration, "\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 7: Hyperparameter Tuning
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat("Step 7: Tuning hyperparameters...\n")
cat("(This may take a few minutes)\n\n")

# Test different eta values
eta_values <- c(0.01, 0.05, 0.1, 0.3)
tuning_results <- data.frame()

for (eta_val in eta_values) {
  params_test <- params_default
  params_test$eta <- eta_val
  
  # Adjust rounds based on eta
  nrounds_test <- ifelse(eta_val <= 0.05, 300, 150)
  
  cv_test <- xgb.cv(
    params = params_test,
    data = dtrain,
    nrounds = nrounds_test,
    nfold = 5,
    metrics = "merror",
    verbose = 0,
    early_stopping_rounds = 10
  )
  
  test_error <- cv_test$evaluation_log[cv_test$best_iteration, "test_merror_mean"]
  test_accuracy <- (1 - test_error) * 100
  
  tuning_results <- rbind(tuning_results, data.frame(
    eta = eta_val,
    best_iteration = cv_test$best_iteration,
    cv_accuracy = test_accuracy
  ))
  
  cat("eta =", eta_val, "â†’ Accuracy:", round(test_accuracy, 1), "%\n")
}

best_eta <- tuning_results$eta[which.max(tuning_results$cv_accuracy)]
cat("\nâœ“ Best eta:", best_eta, "\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 8: Train Final Model
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat("Step 8: Training final model...\n")

params_final <- params_default
params_final$eta <- best_eta

best_nrounds <- tuning_results$best_iteration[tuning_results$eta == best_eta]

set.seed(42)
final_model <- xgb.train(
  params = params_final,
  data = dtrain,
  nrounds = best_nrounds,
  verbose = 0
)

cat("âœ“ Final model trained with", best_nrounds, "trees\n\n")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# STEP 9: Feature Importance
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat("Step 9: Analyzing feature importance...\n\n")

importance <- xgb.importance(
  feature_names = feature_names,
  model = final_model
)

print(importance)

# Plot importance
xgb
